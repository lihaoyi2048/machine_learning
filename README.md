# 机器学习实战笔记


## 第一部分 分类
### 第1章 机器学习基础
### 第2章 k-近邻算法
优点：精度高，对异常值不敏感，无数据输入假定  
缺点：计算复杂度高，空间复杂度高
### 第3章 决策树
优点：计算复杂度不高，输出结果易于理解，对中间值的缺失不敏感，可以处理不相关特征数据  
缺点：可能会产生过度匹配问题

#### 3.1.1 信息增益
熵，定义为信息的期望值。如果待分类的事务可能划分在多个类中，则符号$x_i$的信息定义为：  
$$l(x_i)=-log_2p(x_i)$$  
其中$p(x_i)$是选择该分类的概率。  
$$H = -\sigma_{n-1}^{n}p(x_i)log_2p(x_i)$$，n是分类数目

### 第4章 朴素贝叶斯算法
优点：在数据较少的情况下仍然有效，可以处理多类别问题
缺点：对于输入数据的准备方式较为敏感  
贝叶斯决策理论的核心思想：选择具有最高概率的决策。
用$p1(x,y)$表示数据点$(x,y)$属于类别1的概率，
用$p2(x,y)$表示数据点$(x,y)$属于类别2的概率，对于数据点$(x,y)$，
可用下面的规则来判断它的类别：
- 如果$p1(x,y)>p2(x,y)$，那么类别为1.
- 如果$p1(x,y)<p2(x,y)$，那么类别为2.

$$p(c_j|W)=\frac{p(W|c_j)p(c_j)}{p(W)}$$  
$p(c_j)$:类别$j$的文档数除以总文档数，$W$为词向量。  
根据朴素贝叶斯假设：
$p(W|c_j)=p(w_0,w_1,w_2,\cdot,w_n|c_j)=p(w_0|c_j)p(w_1|c_j)p(w_2|c_j)\cdot p(w_m|c_j)$

伪代码：
```  
计算每个类别中的文档数量
对每篇训练文档：
    对每个类别：
        如果词条出现在文档中 --> 增加该词条的计数值
        增加所有词条的计数值
    对每个类别：
        对每个词条：
            将该词条的数目除以总词条数目得到条件概率
    返回每个类别的条件概率
```


### 第5章 Logistic回归
