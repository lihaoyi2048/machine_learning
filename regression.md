# 第8章 回归分析

## 8.1 线性回归
优点：结果易于理解，计算不复杂
缺点：对非线性数据拟合不好

输入实例$x_i$的特征向量记作

$$x_i=(x_i^{(1)},x_i^{(2)},\cdots,x_i^{(l)},\cdots,x_i^{(n)})^T$$

$x_i^{(l)}$表示样本 $x_i$ 的第 $l$ 个特征。


$$
f(x_i)=w_0 x_i^{(0)} + w_1 x_i^{(1)} + \cdots + w_l x_i^{(l)} + \cdots + w_n x_i^{(n)}=w^Tx_i
$$
其中  $  x_i^0=1 $，

训练数据集：

$$T=\{(x_1,y_1),(x_2,y_2),\cdots,(x_i,y_i),\cdots,(x_{N},y_{N})\}$$

其中$(x_i,y_i), i=1,2,\cdots,N$，称为样本或样本点，$x_i$是输入的观测值，也称为输入或实例，$y_i$是输出的观测值，也称为输出。记矩阵 $X=(x_1,x_2,\cdots,x_N)$，向量 $y=(y_i,y_2,\cdots,y_N)$

损失函数为训练集的平方误差
$$
J(w)=\sum_{i=1}^{N}(y_i - x_i^T w)^2=(y-Xw)^T (y-Xw)
$$

对$w$求导得$X^Ty-X^TXw$，令其等于0，解得：
$$
\hat w = (X^TX)^{-1}X^Ty
$$


## 8.2 局部加权线性回归

Locally Weighted Linear Regression（LWLR），回归系数为：
$$
\hat w=(X^TWX)^{-1}X^TWy
$$
$W$是一个矩阵，用来给每个数据点赋予权重。LWLR使用核函数来对附近的点赋予更高的权重。常用的核函数是高斯核。对应的权重如下：
$$
w(i,i)=exp(\frac{|x^{(i)}-x|}{-2k^2})
$$
这样构建了一个只含对角元素的权重矩阵$W$，并且点$x$和$x^{(i)}$越近，$w(i,i)$就会越大。参数$k$需要指定，决定了对附近的点赋值多大的权重。  

局部加权回归，每次必须在整个训练集上运行，为了做出预测，必须保存所有的训练数据。

## 8.3 岭回归

岭回归，ridge regression，可用来处理特征数多于样本数的情况，现在也用于在估计中加入偏差，从而得到更好的估计。通过引入$\lambda$来限制所有的$W$之和，通过引入惩罚项，能够减少不重要的参数，在统计学中也叫缩减（shrinkage）。

岭回归，就是在矩阵$X^TX$上加入一个 $\lambda I$ 从而使得矩阵非奇异，进而能对$X^TX + \lambda I$求逆。其中矩阵$I$是一个$(n,n)$的单位矩阵，对角线上的元素均为1，其他位置均为0。而$\lambda$是可定义的数值。

岭回归的损失函数
$$
J(w)=\sum_{i=1}^{N} (y_i-x_i^Tw)^2+\lambda||w||_2^2=(y-X^Tw)^T(y-X^Tw)+\lambda w^Tw
$$


对$w$求导，得
$$
\frac{\partial J}{\partial w}=-X^Ty-X^Ty+2X^TXw+2\lambda w
$$
令其等于0，求得回归系数：
$$
\hat w = (X^TX+\lambda I)^{-1}X^Ty
$$

## 8.4 LASSO回归

Lasso回归也叫做线性回归的L1正则化，和岭回归的主要区别是正则化项，岭回归用的是L2正则化，Lasso回归用的是L1正则化。Lasso回归的损失函数为：
$$
J(w)=\sum_{i=1}^{N} (y_i-x_i^Tw)^2+\lambda||w||_1
$$
Lasso回归使得一些绝对值较小的系数直接变为0，因此特别适合用于参数缩减和选择，因而用来估计稀疏参数的的线性模型。但Lasso回归有一个问题，就是它的损失函数不是连续可导的，由于L1范数用的是绝对值之和，导致损失函数有不可导的点。最小二乘法、梯度下降法、牛顿法与拟牛顿法均无法求解，只能使用坐标轴下降法（coordinate descent）和最小角回归法（Least Angle Regression，LARS）求解。

## 8.5 前向逐步回归

```
数据标准化，使其分布满足0均值和单位方差
在每轮迭代过程中：
	设置当前最小误差lowestError为正无穷
	对每个特征：
		增大或缩小
			改变一个系数得到一个新的W
			计算新W下的误差
			如果误差Error小于当前最小误差lowestError:
				设置Wbest等于当前误差
         将W设置成新的Wbeset
```

## 8.6 Elastic Net（弹性网络）

弹性网络结合了岭回归与Lasso回归的方法

损失函数：
$$
J(w)=\sum_{i=1}^{N} (y_i-x_i^Tw)^2+\alpha\lambda||w||_2^2+(1-\alpha)\lambda||w||_1
$$
